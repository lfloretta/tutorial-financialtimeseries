{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Structured data prediction using Cloud ML Engine\n",
    "This notebook illustrates:\n",
    "\n",
    "1. Exploring a BigQuery dataset using Datalab\n",
    "2. Creating datasets for Machine Learning using Dataflow\n",
    "3. Creating a model using the high-level Estimator API \n",
    "4. Training on Cloud ML Engine\n",
    "5. Deploying the model\n",
    "6. Predicting with the model\n",
    "\n",
    "\n",
    "We will create a toy binary classifier to predict if the Standard&Poor 500 index will close positively or negatively. We will build our features using the close values of these indexes:\n",
    "\n",
    "|Index|Country|Closing Time (EST)|Hours Before S&P Close|\n",
    "|---|---|---|---|\n",
    "|[All Ords](https://en.wikipedia.org/wiki/All_Ordinaries)|Australia|0100|15|\n",
    "|[Nikkei 225](https://en.wikipedia.org/wiki/Nikkei_225)|Japan|0200|14|\n",
    "|[Hang Seng](https://en.wikipedia.org/wiki/Hang_Seng_Index)|Hong Kong|0400|12|\n",
    "|[DAX](https://en.wikipedia.org/wiki/DAX)|Germany|1130|4.5|\n",
    "|[FTSE 100](https://en.wikipedia.org/wiki/FTSE_100_Index)|UK|1130|4.5|\n",
    "|[NYSE Composite](https://en.wikipedia.org/wiki/NYSE_Composite)|US|1600|0|\n",
    "|[Dow Jones Industrial Average](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average)|US|1600|0|\n",
    "|[S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index)|US|1600|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Housekeeping \n",
    "Let us start by installing few packages that we are going to need in the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade --force apache-beam[gcp]==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After the installation is done, please restart the session. Now we are ready to set few environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BUCKET = # insert your bucket name (no underscore)\n",
    "PROJECT = #insert your project name\n",
    "REGION = #insert your region (us-central1, europe-west1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BUCKET = 'lf-gcp-demo-eu-w2'\n",
    "PROJECT = 'lf-gcp-demo'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# create a bucket\n",
    "gsutil mb -l ${REGION} -p ${PROJECT} gs://${BUCKET}\n",
    "# copy file from the vm to the newly created bucket\n",
    "gsutil -m cp data/financialtimeseries/all_data.csv gs://${BUCKET}/data/financialtimeseries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gcs_data_dir = 'gs://{0}/data/financialtimeseries/'.format(BUCKET)\n",
    "gcs_model_dir = 'gs://{0}/ml-models/financialtimeseries/'.format(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you are rerunning the notebook and you are willing to restart from scratch de-comment the statements in the following block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#gsutil -m rm -rf gs://${BUCKET}/ml-models/financialtimeseries/*\n",
    "#gsutil -m rm -rf gs://${BUCKET}/data/financialtimeseries/big_data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a dataset and a table by importing the data in gs://${BUCKET}/data/financialtimeseries/all_data.csv. Instructions are [here](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exploring a BigQuery dataset using Datalab\n",
    "\n",
    "Now that we have the data in BigQuey we can start exploring them. Let us start by writing a query to find the maximum close value for the S&P500 index. \n",
    "\n",
    "[Here](https://cloud.google.com/bigquery/docs/reference/standard-sql/) the reference for the Standard SQL in BigQuery and [here](http://googledatalab.github.io/pydatalab/google.datalab%20Commands.html) the information about the magic %%bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the Google Charts api to plot the evolution of S&P500. You can get inspiration [here](https://cloud.google.com/bigquery/docs/visualize-datalab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the Google Charts api to plot the rolling average of S&P500's close value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using BigQuery to perform complex calculations and aggregations is very powerful because it allows to work with PB of data. However, it does not allow to easily slice and dice the data to find interesting patterns. The usual approach is to sample the data and load them in [pandas](https://pandas.pydata.org/)' dataframes. \n",
    "\n",
    "In our case, we do not need to sample the data as our database fits in mememory. You can get inspiration [here](https://cloud.google.com/bigquery/docs/visualize-datalab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once you have laod the data, plot :\n",
    "1. the time evolution of the all indexes normalized by their maximum close value\n",
    "2. their [autocorrelations](https://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-autocorrelation)\n",
    "3. a [pair plot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Usually, when dealing with finacial timeseries the value at time $t$, $V_t$, is replaced by the log return $r_t = \\log \\big ( \\frac{V_t}{V_{t-1}} \\big )$. What does it changes in our case? Recreate the plots of before using the log returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating datasets for Machine Learning using Dataflow\n",
    "\n",
    "For building our toy binary classifier to predict if the Standard&Poor 500 index will close positively or negatively, we are going to use the following features:\n",
    "1. the close value of S&P 500 of the day before\n",
    "1. the close value of S&P 500 of two day before\n",
    "1. the close value of Dow Jones Industrial Average of the day before\n",
    "1. the close value of Dow Jones Industrial Average of two day before\n",
    "1. the close value of NYSE Composite of the day before\n",
    "1. the close value of NYSE Composite of two day before\n",
    "1. the close value of FTSE 100 of the same day\n",
    "1. the close value of FTSE 100 of the day before\n",
    "1. the close value of DAX of the same day\n",
    "1. the close value of DAX of the day before\n",
    "1. the close value of Hang Seng of the same day\n",
    "1. the close value of Hang Seng of the day before\n",
    "1. the close value of Nikkei 225 of the same day\n",
    "1. the close value of Nikkei 225 of the day before\n",
    "1. the close value of All Ords of the same day\n",
    "1. the close value of All Ords of the day before\n",
    "\n",
    "The first step to train our model is to prepare the training data. We are going to use [Cloud Dataflow](https://cloud.google.com/dataflow/), the GCP managed version of [Apache Beam](https://beam.apache.org/), to read the data from BigQuery data and write them out as CSV files in [Google Cloud Storage](https://cloud.google.com/storage/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The skeleton of our pipeline is going to be the following\n",
    "\n",
    "```python\n",
    "def run_pipeline():\n",
    "    \"\"\"Function that runs the pipeline in Dataflow\"\"\"\n",
    "    job_name = 'preprocess-financialtimeseries-data' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(out_dir, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(out_dir, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'region' : REGION,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True\n",
    "    }\n",
    "  \n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    RUNNER = 'DataflowRunner'\n",
    "  \n",
    "    with beam.Pipeline(RUNNER, options=opts) as pipeline:  \n",
    "      \n",
    "      for step in ['train', 'eval']:\n",
    "          if step == 'train':\n",
    "              source_query = 'SELECT * FROM ({}) WHERE MOD(hashdate,5) < 4'.format(query)\n",
    "          else:\n",
    "              source_query = 'SELECT * FROM ({}) WHERE MOD(hashdate,5) = 4'.format(query)\n",
    "\n",
    "          sink_location = os.path.join(out_dir, '{}-data'.format(step))\n",
    "\n",
    "          (pipeline \n",
    "             | '{} - Read from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "             | '{} - Process single row'.format(step) >> beam.ParDo(EmitShiftedValues())\n",
    "             | '{} - Group by date'.format(step) >> beam.GroupByKey()\n",
    "             | '{} - Create output row'.format(step) >> beam.FlatMap(create_output_row)\n",
    "             | '{} - Write to GCS '.format(step) >> beam.io.Write(beam.io.WriteToText(sink_location,\n",
    "                                                                  file_name_suffix='.csv',\n",
    "                                                                  num_shards=5))\n",
    "          )\n",
    "    \n",
    "   \n",
    "    job = pipeline.run()\n",
    "```\n",
    "\n",
    "\n",
    "The starting point is a standard sql query saved in the string variable ```query```. The field ```hashdate``` is used to split the data in training and evaluation (see [here](https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning).)\n",
    "\n",
    "The output of this first step is a [PCollection](https://beam.apache.org/documentation/programming-guide/#pcollection-characteristics) of dictionaries whose keys are the fields of ```query```.\n",
    "\n",
    "```EmitShiftedValues``` is a subclass of [```beam.DoFn```](https://beam.apache.org/documentation/programming-guide/#pardo) which implements the Map phase of a MapReduce algorithm.\n",
    "\n",
    "[```beam.GroupByKey()``` ](https://beam.apache.org/documentation/programming-guide/#groupbykey) implements the Reduce phase of a MapReducte algorithm creating a PCollection.\n",
    "\n",
    "[```beam.FlatMap```](https://beam.apache.org/documentation/programming-guide) applies to each element of the PCollection the function ```create_output_row``` creating a new PCollection. The scope of ```create_output_row``` is to create a line of the fineal csv files.\n",
    "\n",
    "The last step writes out the data in GCS.\n",
    "\n",
    "Now that you know the plan, implement the diffrenet pieces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    import apache_beam as beam\n",
    "    print(beam.__version__)\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Write here your query. You can test it in the BigQuery UI.\n",
    "query = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "out_dir = gcs_data_dir + \"big_data\"\n",
    "\n",
    "  \n",
    "  \n",
    "class EmitShiftedValues(beam.DoFn):\n",
    "\n",
    "  def process(self, element):\n",
    "    output = []\n",
    "    # Add to output the tuples that you want to emit\n",
    "    \n",
    "    for pair in output:\n",
    "      yield pair\n",
    "\n",
    "      \n",
    "def create_output_row(pair):\n",
    "  \"\"\"Function that creates a row of the output\n",
    "     Args:\n",
    "        pairs : (key, values)\n",
    "     Yield:\n",
    "        a line of a csv file\n",
    "  \"\"\"\n",
    "  # Write here code to create the csv row\n",
    "  output_row = ''\n",
    "  \n",
    "  yield output_row\n",
    "  \n",
    "      \n",
    "def run_pipeline():\n",
    "    \"\"\"Function that runs the pipeline in Dataflow\"\"\"\n",
    "    job_name = 'preprocess-financialtimeseries-data' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(out_dir, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(out_dir, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'region' : REGION,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True\n",
    "    }\n",
    "  \n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    RUNNER = 'DataflowRunner'\n",
    "  \n",
    "    with beam.Pipeline(RUNNER, options=opts) as pipeline:  \n",
    "      \n",
    "      for step in ['train', 'eval']:\n",
    "          if step == 'train':\n",
    "              source_query = 'SELECT * FROM ({}) WHERE MOD(hashdate,5) < 4'.format(query)\n",
    "          else:\n",
    "              source_query = 'SELECT * FROM ({}) WHERE MOD(hashdate,5) = 4'.format(query)\n",
    "\n",
    "          sink_location = os.path.join(out_dir, '{}-data'.format(step))\n",
    "\n",
    "          (pipeline \n",
    "             | '{} - Read from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "             | '{} - Process single row'.format(step) >> beam.ParDo(EmitShiftedValues())\n",
    "             | '{} - Group by date'.format(step) >> beam.GroupByKey()\n",
    "             | '{} - Create output row'.format(step) >> beam.FlatMap(create_output_row)\n",
    "             | '{} - Write to GCS '.format(step) >> beam.io.Write(beam.io.WriteToText(sink_location,\n",
    "                                                                  file_name_suffix='.csv',\n",
    "                                                                  num_shards=5))\n",
    "          )\n",
    "    \n",
    "   \n",
    "    job = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have our pipeline, we just need to submit our job to Cloud Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us check the data created by our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/data/financialtimeseries/big_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a model using the high-level Estimator API\n",
    "\n",
    "Before we start implementing our model, let us define a baseline. We need a baseline to judge how well we are doing. So what is your baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let us define our model. You can find inspiration [here](https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/02%20-%20Classification/05.0%20-%20Classification%20Example%20-%20%20Census%20Income%20Prediction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Data Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Your Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run Experiment\n",
    "\n",
    "Notice that you have local file for training and evaluation at data/financialtimeseries/train-data.csv and data/financialtimeseries/eval-data.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training on Cloud ML Engine\n",
    "\n",
    "Submit a training job to CMLE requires to create a python package as CMLE will pip installed it. [Here](https://cloud.google.com/ml-engine/docs/packaging-trainer) are the instructions. \n",
    "\n",
    "To facilitate your task I have copied in the directory mk-packeges/trainer the example taken for the officil [GoogleCloudPlatform](https://github.com/GoogleCloudPlatform/cloudml-samples) github repository.\n",
    "\n",
    "Once you have finished with writing your code, the first step is test the package [locally](https://cloud.google.com/sdk/gcloud/reference/ml-engine/local/train). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#to adapt according to what you have achieved\n",
    "\n",
    "MODULE_NAME=trainer.task \n",
    "PACKAGE_PATH=ml-packages/trainer\n",
    "\n",
    "gcloud ml-engine local train --module-name=$MODULE_NAME \\\n",
    "                             --package-path=$PACKAGE_PATH \\\n",
    "                             -- \\\n",
    "                             --your-flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your code runs locally, we can submit our training job to [CMLE](https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=${REGION}\n",
    "TIER=BASIC # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=${BUCKET}\n",
    "\n",
    "MODEL_NAME=\"sp500_classifier\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/trainer\n",
    "MODEL_DIR=gs://${BUCKET}/models/${MODEL_NAME}\n",
    "\n",
    "#remove model directory, if you don't want to resume training, or if you have changed the model structure\n",
    "#gsutil -m rm -r ${MODEL_DIR}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "       --job-dir=${MODEL_DIR} \\\n",
    "       --runtime-version=1.4 \\\n",
    "       --region=${REGION} \\\n",
    "       --scale-tier=${TIER} \\\n",
    "       --module-name=trainer.task \\\n",
    "       --package-path=${PACKAGE_PATH} \\\n",
    "       -- \\\n",
    "       --your-flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REGION=${REGION}\n",
    "BUCKET=${BUCKET}\n",
    "\n",
    "MODEL_NAME=\"sp500_classifier\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "\n",
    "MODEL_BINARIES=<directory on gcs containing the binaries>\n",
    "\n",
    "# deploy model to GCP\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions=${REGION}\n",
    "\n",
    "#deploy model version\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model=${MODEL_NAME} --origin=${MODEL_BINARIES} --runtime-version=1.4\n",
    "\n",
    "echo  ${MODEL_NAME} ${MODEL_VERSION} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with the model\n",
    "\n",
    "Once we have created our version, we could use it for getting predictions [online](https://cloud.google.com/ml-engine/docs/online-predict) or in [batch mode](https://cloud.google.com/ml-engine/docs/batch-predict).\n",
    "\n",
    "We will limit ourself to testing online prediction with gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME=\"sp500_classifier\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "\n",
    "# invoke deployed model to make prediction given new data instances\n",
    "gcloud ml-engine predict --model=${MODEL_NAME} --version=${MODEL_VERSION} --json-instances=data/financialtimeseries/new-data.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
